{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:06:40.569588Z",
     "start_time": "2024-03-04T18:06:40.519604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5b6afa44bdeb50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:06:42.160418Z",
     "start_time": "2024-03-04T18:06:40.532684Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import transformers\n",
    "import json\n",
    "import logging\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "from prompt import make_full_source, make_full_target, make_answer_only_source, make_answer_only_target\n",
    "\n",
    "from util import save_output\n",
    "\n",
    "\n",
    "def _tokenize_fn(strings: list[str],\n",
    "                 tokenizer: transformers.PreTrainedTokenizer) -> dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ) for text in tqdm(strings)\n",
    "    ]\n",
    "    input_ids = labels = [\n",
    "        tokenized.input_ids[0] for tokenized in tokenized_list\n",
    "    ]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "        for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "        sources: list[str],\n",
    "        targets: list[str],\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in tqdm(zip(labels, sources_tokenized[\"input_ids_lens\"])):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, training_objective: str):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        # list_data_dict = json.load(open(data_path, \"r\"))\n",
    "        list_data_dict = []\n",
    "        with open(data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                list_data_dict.append(json.loads(line))\n",
    "\n",
    "        sources = []\n",
    "        targets = []\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        if training_objective == \"full\":\n",
    "            for example in list_data_dict:\n",
    "                source = make_full_source(example)\n",
    "                target = make_full_target(example)\n",
    "                target += f\"{tokenizer.eos_token}\"\n",
    "                sources.append(source)\n",
    "                targets.append(target)\n",
    "        elif training_objective == \"answer-only\":\n",
    "            for example in list_data_dict:\n",
    "                source = make_answer_only_source(example)\n",
    "                target = make_answer_only_target(example, eos_token=tokenizer.eos_token)\n",
    "                sources.append(source)\n",
    "                targets.append(target)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        logging.warning(f\"Number of source examples: {len(sources)}\")\n",
    "        logging.warning(f\"Number of target examples: {len(targets)}\")\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorForSupervisedDataset:\n",
    "#     \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "# \n",
    "#     tokenizer: transformers.PreTrainedTokenizer\n",
    "# \n",
    "#     def __call__(self, instances: list[dict]) -> dict[str, torch.Tensor]:\n",
    "#         input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "#                                   for key in (\"input_ids\", \"labels\"))\n",
    "#         input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "#             input_ids,\n",
    "#             batch_first=True,\n",
    "#             padding_value=self.tokenizer.pad_token_id)\n",
    "#         labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "#                                                  batch_first=True,\n",
    "#                                                  padding_value=IGNORE_INDEX)\n",
    "#         return dict(\n",
    "#             input_ids=input_ids,\n",
    "#             labels=labels,\n",
    "#             attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "#         )\n",
    "\n",
    "# now to import train_test_split from torch.utils.data.random_split\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "@save_output\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                data_args) -> dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer,\n",
    "                                      data_path=data_args['data_path'],\n",
    "                                      training_objective=data_args['training_objective'])\n",
    "    train_dataset, eval_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.9),\n",
    "                                                               len(train_dataset) - int(len(train_dataset) * 0.9)])\n",
    "\n",
    "    return dict(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9caacd1f6171f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:06:43.135696Z",
     "start_time": "2024-03-04T18:06:42.165727Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import set_seed\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "def make_trainer(model: transformers.PreTrainedModel,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_module: dict,\n",
    "                 training_args: TrainingArguments) -> Trainer:\n",
    "    \"\"\"Make a trainer for fine-tuning.\"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=training_args.warmup_steps,\n",
    "                                                num_training_steps=training_args.num_train_epochs)\n",
    "    set_seed(training_args.seed)\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_module[\"data_collator\"],\n",
    "        train_dataset=data_module[\"train_dataset\"],\n",
    "        eval_dataset=data_module[\"eval_dataset\"],\n",
    "        optimizers=(optimizer, scheduler),\n",
    "        args=training_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6340fb59acce1fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:07:33.202357Z",
     "start_time": "2024-03-04T18:06:43.133081Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ffbd937061430fbe03892ebc93d69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "# get model max input size\n",
    "model_max_length = model.config.max_position_embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, model_max_length=model_max_length,\n",
    "                                          padding_side=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e50968f5421ff8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:07:33.213717Z",
     "start_time": "2024-03-04T18:07:33.204847Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"pad_token\": DEFAULT_PAD_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a0f8d9906f3b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:07:33.214231Z",
     "start_time": "2024-03-04T18:07:33.209817Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path(\"/lfs/skampere1/0/kaif\")\n",
    "\n",
    "data_args = {\n",
    "    \"data_path\": root_dir / \"data/selfee-train.json\",\n",
    "    \"training_objective\": \"full\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653a33a001b83037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:07:33.222850Z",
     "start_time": "2024-03-04T18:07:33.212492Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import json\n",
    "# \n",
    "# dataset = load_dataset('kaist-ai/selfee-train')[\"train\"]\n",
    "# \n",
    "# # Write the merged JSON to a new file\n",
    "# # dataset.to_json(\"~/data/selfee-train\",force_ascii=False) # ValueError: 'index=True' is only valid when 'orient' is 'split', 'table', 'index', or 'columns'.\n",
    "# dataset.to_json(root_dir / \"data/selfee-train.json\", force_ascii=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbbd2deafdadfa9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:07:55.084685Z",
     "start_time": "2024-03-04T18:07:33.216710Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading output from /lfs/skampere1/0/kaif/data/selfee-train_preprocessed.pkl\n"
     ]
    }
   ],
   "source": [
    "data_module = make_supervised_data_module(tokenizer, data_args, load=True, override=False,\n",
    "                                          output_path=root_dir / \"data/selfee-train_preprocessed.pkl\")\n",
    "\n",
    "# train_dataloader = DataLoader(train_ds, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "# eval_dataloader = DataLoader(eval_ds, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b977851c9b56825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:07:55.306453Z",
     "start_time": "2024-03-04T18:07:55.130764Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _collate_fn(batch: list[dict]) -> dict[str, torch.Tensor]:\n",
    "    input_ids, labels = tuple([example[key] for example in batch]\n",
    "                              for key in (\"input_ids\", \"labels\"))\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids,\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                             batch_first=True,\n",
    "                                             padding_value=IGNORE_INDEX)\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(data_module[\"train_dataset\"], shuffle=True, collate_fn=_collate_fn,\n",
    "                              batch_size=1, pin_memory=True)\n",
    "eval_dataloader = DataLoader(data_module[\"eval_dataset\"], collate_fn=_collate_fn, batch_size=1,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "548c951a9d14691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:07:55.739129Z",
     "start_time": "2024-03-04T18:07:55.306337Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 57,344 || all params: 6,910,423,040 || trainable%: 0.0008298189512866639\n"
     ]
    }
   ],
   "source": [
    "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model\n",
    "\n",
    "prompt_tuning_init_text = \"Classify if the tweet is a complaint or no complaint.\\n\"\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=len(tokenizer(prompt_tuning_init_text)[\"input_ids\"]),\n",
    "    prompt_tuning_init_text=prompt_tuning_init_text,\n",
    "    tokenizer_name_or_path=model_name,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9572761b85137399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T18:07:55.741060Z",
     "start_time": "2024-03-04T18:07:55.738517Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = 3e-2\n",
    "num_epochs = 50\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d2858edb8d9d5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-04T18:07:55.739585Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 168/160497 [00:31<5:49:17,  7.65it/s]"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838a3449b785ce4",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=root_dir / f\"output/finetune/{model_name}\",\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     evaluation_strategy=\"no\",\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=5000,\n",
    "#     save_total_limit=1,\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.,\n",
    "#     warmup_ratio=0.03,\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     logging_steps=1,\n",
    "#     bf16=True,\n",
    "#     tf32=True,\n",
    "#     gradient_checkpointing=True,\n",
    "# )\n",
    "# \n",
    "# trainer = make_trainer(model, tokenizer, data_module, training_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
