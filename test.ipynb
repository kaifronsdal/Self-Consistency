{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8b0c2f73e1646c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T19:22:25.252723Z",
     "start_time": "2024-03-05T19:22:24.941538Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lfs/skampere1/0/kaif/conda/envs/TheoremSense/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba13fe1-54fd-499f-9e9c-fdc0e05f25fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:51:54.363502Z",
     "start_time": "2024-03-12T20:51:52.345556Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from dataset import load_datasets, BOXED_ANSWERS_DATASETS, get_boxed_answer\n",
    "from latex_formater import latex_deformat\n",
    "from prompt import generate_nshot_prompts, generate_nshot_prompts_from_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fef518d751870fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-10T05:12:17.288349Z",
     "start_time": "2024-03-10T05:12:17.284888Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import pipeline\n",
    "# \n",
    "# # model_name = \"kaist-ai/selfee-7b-delta\"\n",
    "# pipe = pipeline('text-generation', model=model_name, tokenizer=model_name, device_map=\"auto\",\n",
    "#                 torch_dtype=torch.bfloat16, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ddcfe5-fa56-4779-9025-ca7e1a390ba9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:09.856414Z",
     "start_time": "2024-03-12T20:51:54.360055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-12 13:51:54 config.py:577] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-12 13:51:54 llm_engine.py:79] Initializing an LLM engine with config: model='google/gemma-2b-it', tokenizer='google/gemma-2b-it', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 03-12 13:51:57 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "INFO 03-12 13:51:59 llm_engine.py:337] # GPU blocks: 231898, # CPU blocks: 14563\n",
      "INFO 03-12 13:52:00 model_runner.py:676] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-12 13:52:00 model_runner.py:680] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-12 13:52:09 model_runner.py:748] Graph capturing finished in 8 secs.\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=2048)\n",
    "# model_name = \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "# model_name = \"Qwen/Qwen1.5-7B-Chat-AWQ\"\n",
    "# model_name = \"kaist-ai/selfee-13b-delta\"\n",
    "# model_name = \"llm-agents/tora-13b-v1.0\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# dtype = \"bfloat16\"\n",
    "dtype = \"float16\"\n",
    "llm = LLM(model=model_name, trust_remote_code=True, dtype=dtype)  #tensor_parallel_size=2,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4464b08ce16892",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    result = llm.generate([\"What is 2+2?\" * 100], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ce9336-d3e6-4ad7-940f-46b11d4e8e0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:15.899056Z",
     "start_time": "2024-03-12T20:52:09.833155Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "datasets_raw = load_datasets(BOXED_ANSWERS_DATASETS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80593f1308a3f14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:15.913822Z",
     "start_time": "2024-03-12T20:52:15.894130Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load the name maps\n",
    "with open('name_maps.json', 'r') as f:\n",
    "    name_maps = json.load(f)\n",
    "    DATASET_MAP = name_maps['DATASET_MAP']\n",
    "    MODEL_MAP = name_maps['MODEL_MAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c432d83787b08f33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:16.909584Z",
     "start_time": "2024-03-12T20:52:15.896380Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NSHOT_MAP = {\n",
    "#     'Algebra': [0, 1, 2],\n",
    "#     'Counting and Probability': [0, 1, 2],\n",
    "#     'Geometry': [1, 6, 8],\n",
    "#     'Intermediate Algebra': [0, 1, 2],\n",
    "#     'Number Theory': [0, 1, 2],\n",
    "#     'Prealgebra': [0, 1, 2],\n",
    "#     'Precalculus': [4, 6, 8],\n",
    "#     'GSM8K': [0, 1, 2]\n",
    "# }\n",
    "\n",
    "# datasets = {DATASET_MAP[dataset['name']]: generate_nshot_prompts(dataset['data']['train'], n=NSHOT_MAP[DATASET_MAP[dataset['name']]]) for dataset in datasets_raw}\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "datasets = {\n",
    "    DATASET_MAP[dataset['name']]: generate_nshot_prompts_from_template(dataset['data']['train'], n=3)\n",
    "    for dataset in datasets_raw\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "num_samples = 100\n",
    "datasets_subset = {k: np.random.choice(v, num_samples, replace=False) for k, v in datasets.items()}\n",
    "# dtype = 'Algebra'\n",
    "# datasets_subset = {dtype: datasets_subset[dtype]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1241d65ef7c1ede2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T05:58:21.831715Z",
     "start_time": "2024-03-12T05:58:21.814779Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Let \\\\[f(x) = \\\\left\\\\{\\n\\\\begin{array}{cl} ax+3, &\\\\text{ if }x>2, \\\\\\\\\\nx-5 &\\\\text{ if } -2 \\\\le x \\\\le 2, \\\\\\\\\\n2x-b &\\\\text{ if } x <-2.\\n\\\\end{array}\\n\\\\right.\\\\]Find $a+b$ if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).'}, {'role': 'assistant', 'content': 'For the piecewise function to be continuous, the cases must \"meet\" at $2$ and $-2$. For example, $ax+3$ and $x-5$ must be equal when $x=2$. This implies $a(2)+3=2-5$, which we solve to get $2a=-6 \\\\Rightarrow a=-3$. Similarly, $x-5$ and $2x-b$ must be equal when $x=-2$. Substituting, we get $-2-5=2(-2)-b$, which implies $b=3$. So $a+b=-3+3=\\\\boxed{0}$.'}, {'role': 'user', 'content': 'FINAL ANSWER: '}, {'role': 'assistant', 'content': '0'}, {'role': 'user', 'content': 'A rectangular band formation is a formation with $m$ band members in each of $r$ rows, where $m$ and $r$ are integers. A particular band has less than 100 band members. The director arranges them in a rectangular formation and finds that he has two members left over. If he increases the number of members in each row by 1 and reduces the number of rows by 2, there are exactly enough places in the new formation for each band member. What is the largest number of members the band could have?'}, {'role': 'assistant', 'content': 'Let $x$ be the number of band members in each row for the original formation, when two are left over.  Then we can write two equations from the given information: $$rx+2=m$$ $$(r-2)(x+1)=m$$ Setting these equal, we find: $$rx+2=(r-2)(x+1)=rx-2x+r-2$$ $$2=-2x+r-2$$ $$4=r-2x$$ We know that the band has less than 100 members.  Based on the first equation, we must have $rx$ less than 98.  We can guess and check some values of $r$ and $x$ in the last equation.  If $r=18$, then $x=7$, and $rx=126$ which is too big.  If $r=16$, then $x=6$, and $rx=96$, which is less than 98.  Checking back in the second formation, we see that $(16-2)(6+1)=14\\\\cdot 7=98$ as it should.  This is the best we can do, so the largest number of members the band could have is $\\\\boxed{98}$.'}, {'role': 'user', 'content': 'FINAL ANSWER: '}, {'role': 'assistant', 'content': '98'}, {'role': 'user', 'content': 'What is the degree of the polynomial $(4 +5x^3 +100 +2\\\\pi x^4 + \\\\sqrt{10}x^4 +9)$?'}, {'role': 'assistant', 'content': \"This polynomial is not written in standard form.  However, we don't need to write it in standard form, nor do we need to pay attention to the coefficients.  We just look for the exponents on $x$.  We have an $x^4$ term and no other term of higher degree, so $\\\\boxed{4}$ is the degree of the polynomial.\"}, {'role': 'user', 'content': 'FINAL ANSWER: '}, {'role': 'assistant', 'content': '4'}, {'role': 'user', 'content': 'Evaluate $\\\\left\\\\lceil3\\\\left(6-\\\\frac12\\\\right)\\\\right\\\\rceil$.'}, {'role': 'assistant', 'content': 'Firstly, $3\\\\left(6-\\\\frac12\\\\right)=18-1-\\\\frac12=17-\\\\frac12$.  Because $0\\\\le\\\\frac12<1$, we have $\\\\left\\\\lceil17-\\\\frac12\\\\right\\\\rceil=\\\\boxed{17}$.'}, {'role': 'user', 'content': 'FINAL ANSWER: '}, {'role': 'assistant', 'content': '17'}, {'role': 'user', 'content': 'The sum of the squares of two positive integers is 193. The product of the two integers is 84. What is the sum of the two integers?'}, {'role': 'user', 'content': 'FINAL ANSWER: '}]\n"
     ]
    }
   ],
   "source": [
    "print(datasets_subset['Algebra'][0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1c292dd7b0376d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:16.914205Z",
     "start_time": "2024-03-12T20:52:16.910826Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(prompts['Algebra'][1]['question'])\n",
    "# prompt the model for the answers for the question\n",
    "from util import save_output\n",
    "\n",
    "\n",
    "@save_output\n",
    "def generate(chat_prompts, model, tokenizer, sampling_params):\n",
    "    # assume each chat_prompt is the same length\n",
    "    chat_pattern = chat_prompts[0]\n",
    "    last_assistant_idx = [i for i, p in enumerate(chat_pattern) if p['role'] == \"assistant\"][-1]\n",
    "    # get the current chat prompts\n",
    "    curr_prompts = [p[:last_assistant_idx + 1] for p in chat_prompts]\n",
    "    for i in range(last_assistant_idx + 1, len(chat_pattern)):\n",
    "        # add the next chat_prompts to the current prompt\n",
    "        curr_prompts = [cp + [p[i]] for cp, p in zip(curr_prompts, chat_prompts)]\n",
    "        # convert to chat template and generate the model's response\n",
    "        prompts = [tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=True) for p in\n",
    "                   curr_prompts]\n",
    "        outputs = model.generate(prompts, sampling_params)\n",
    "        # add the model's response to the current prompts\n",
    "        curr_prompts = [\n",
    "            p + [{\"role\": \"assistant\", \"content\": o.outputs[0].text}]\n",
    "            for p, o in zip(curr_prompts, outputs)\n",
    "        ]\n",
    "    return curr_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ce4ca2d271534e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-10T23:32:52.364087Z",
     "start_time": "2024-03-10T23:32:44.033280Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.15s/it]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'role': 'user', 'content': 'Hello, how are you?'}, {'role': 'assistant', 'content': \"I'm doing great. How can I help you today?\"}, {'role': 'user', 'content': \"I'd like to show off how chat templating works!\"}, {'role': 'assistant', 'content': 'Sure! I\\'d be happy to help you demonstrate chat templating. Chat templating allows you to create dynamic messages using placeholders that are replaced with actual values at runtime. This can be useful for creating personalized messages, displaying data from a database, or handling specific user inputs. Here\\'s a simple example to get you started:\\n\\n1. **Template Syntax:**\\n   Most chat platforms or libraries use a specific syntax for template placeholders. Let\\'s take a common example of a simple template for a user\\'s name:\\n\\n   ```\\n   Hi, {{user_name}}!\\n   ```\\n\\n2. **Data to Replace Placeholders:**\\n   You need to provide the actual data that will replace the placeholders. For instance, if the user\\'s name is \"Alice\", you would pass it as an argument:\\n\\n   ```python\\n   user_name = \"Alice\"\\n   ```\\n\\n3. **Rendering the Template:**\\n   Now, you can use a chat library or API to render the template with the data:\\n\\n   ```python\\n   message = template.render(user_name=user_name)\\n   ```\\n\\n4. **Output:**\\n   The final output would be the dynamically generated message:\\n\\n   ```\\n   Hi, Alice!\\n   ```\\n\\n5. **Advanced Templates:**\\n   Some chat templates might support more complex structures, like loops, conditionals, or even dynamic content based on user actions. For example, you might have a template for a to-do list:\\n\\n   ```\\n   Today\\'s tasks:\\n   - {{task1}}\\n   - {{if task2}}\\n     - {{task2}}\\n   {{/if}}\\n   ```\\n\\n   With data like this:\\n\\n   ```python\\n   tasks = [\"Buy groceries\", \"Finish project\"]\\n   task1 = tasks[0]\\n   task2 = tasks[1] if tasks else None\\n   ```\\n\\n   The output would look like:\\n\\n   ```\\n   Today\\'s tasks:\\n   - Buy groceries\\n   - Finish project (if the second task exists)\\n   ```\\n\\nI hope this helps you showcase the basics of chat templating! Let me know if you want to see an example with a specific chat platform or library.'}, {'role': 'user', 'content': 'Copy the last model response.'}, {'role': 'assistant', 'content': 'Sure! Here\\'s a more detailed example using the popular Python library `jinja2` for chat templating:\\n\\n1. **Install `jinja2`:**\\n   If you haven\\'t installed it yet, you can do so with pip:\\n   ```bash\\n   pip install jinja2\\n   ```\\n\\n2. **Import and Configure:**\\n   First, create a `Templater` object and set up a template with placeholders:\\n\\n   ```python\\n   from jinja2 import Template\\n\\n   # Define your template\\n   template_string = \"\"\"\\n   Welcome to our system, {{user_name}}!\\n   Your tasks today are:\\n   - {{task1}}\\n   - {{#if task2}}\\n     - {{task2}}\\n   {{/if}}\\n   \"\"\"\\n\\n   # Create a template from the string\\n   template = Template(template_string)\\n   ```\\n\\n3. **Pass data to the template:**\\n   Now, pass the user\\'s name and tasks to the `render` method:\\n\\n   ```python\\n   user_name = \"Alice\"\\n   tasks = [\"Buy groceries\", \"Finish project\"]\\n\\n   # Render the template\\n   rendered_message = template.render(user_name=user_name, task1=task1, task2=task2)\\n   ```\\n\\n4. **Output the rendered message:**\\n   The `rendered_message` will now contain the dynamic output based on the provided data.\\n\\n   ```python\\n   print(rendered_message)\\n   ```\\n\\nOutput:\\n```\\nWelcome to our system, Alice!\\nYour tasks today are:\\n- Buy groceries\\n- Finish project\\n```\\n\\nThis demonstrates a simple chat templating example using `jinja2`. You can expand this to more complex templates as needed. If you\\'re using a different chat platform or library, the process might vary slightly, but the concept is similar.'}], [{'role': 'user', 'content': 'Hello, how are you?'}, {'role': 'assistant', 'content': \"I'm doing great. How can I help you today?\"}, {'role': 'user', 'content': \"I'd like to show off how chat templating works!\"}, {'role': 'assistant', 'content': 'Sure, I\\'d be happy to help you demonstrate chat templating. Chat templating is a way to create dynamic conversations in chat interfaces, allowing you to generate responses based on user inputs or predefined templates. Here\\'s a simple example to get you started:\\n\\nLet\\'s say you\\'re building a customer support chatbot for an e-commerce website, and you want to respond to common queries with pre-written messages. You can create a template that includes placeholders for user inputs, and then replace them with actual values when the bot processes the message.\\n\\nHere\\'s a basic template in a popular chat system, like many messaging platforms:\\n\\n```chat\\n{{greeting}}\\nHello, {{user_name}}! How can I assist you today? You recently bought {{product_name}}.\\n\\n{{options}}\\n1. Returns\\n2. Shipping status\\n3. Questions about the product\\n\\nIf you have any issues, please provide your order number: {{order_number}}\\n\\n{{help}}\\nFor assistance, please reply with the option number.\\n\\n{{farewell}}\\nThank you! Have a great day!\\n```\\n\\nWhen a user types a message like:\\n\\n```\\nHi, I bought the iPhone 13 and the battery life is too short.\\n```\\n\\nThe chatbot would replace the placeholders with the actual data:\\n\\n- `greeting`: \"Hello, user!\"\\n- `user_name`: \"user\"\\n- `product_name`: \"iPhone 13\"\\n- `order_number`: \"your_order_number\"\\n- `options`: \"1. Returns\"\\n- `help`: \"For returns, please reply with \\'1\\'\"\\n- `farewell`: \"Thank you! Have a great day!\"\\n\\nThis way, you can save time and ensure consistency in your responses without having to write each one manually. If you\\'re using a specific chat platform or library, the syntax might differ, but the concept is the same. Would you like to see an example using a specific chat API or platform?'}, {'role': 'user', 'content': 'Copy the last model message.'}, {'role': 'assistant', 'content': 'Of course! Here\\'s the last model message we created using the template structure:\\n\\n```\\nIf you have any issues, please provide your order number: {{order_number}}\\n\\nFor assistance, please reply with the option number.\\n```\\n\\nThis message is waiting for the user to provide their order number and then ask which option they need help with by replying with the corresponding number. If the user sends a message like \"I need to return my phone, my order number is 1234567890\", the chatbot would replace the `{{order_number}}` placeholder with \"1234567890\" and adjust the response accordingly.'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test the generate function\n",
    "chat_prompts = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "        {\"role\": \"user\", \"content\": \"Copy the last model response.\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "        {\"role\": \"user\", \"content\": \"Copy the last model message.\"}\n",
    "    ]\n",
    "]\n",
    "outputs = generate(chat_prompts, llm, tokenizer, sampling_params)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6cfcff8b99b9c13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-10T23:34:45.239059Z",
     "start_time": "2024-03-10T23:34:45.194534Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a more detailed example using the popular Python library `jinja2` for chat templating:\n",
      "\n",
      "1. **Install `jinja2`:**\n",
      "   If you haven't installed it yet, you can do so with pip:\n",
      "   ```bash\n",
      "   pip install jinja2\n",
      "   ```\n",
      "\n",
      "2. **Import and Configure:**\n",
      "   First, create a `Templater` object and set up a template with placeholders:\n",
      "\n",
      "   ```python\n",
      "   from jinja2 import Template\n",
      "\n",
      "   # Define your template\n",
      "   template_string = \"\"\"\n",
      "   Welcome to our system, {{user_name}}!\n",
      "   Your tasks today are:\n",
      "   - {{task1}}\n",
      "   - {{#if task2}}\n",
      "     - {{task2}}\n",
      "   {{/if}}\n",
      "   \"\"\"\n",
      "\n",
      "   # Create a template from the string\n",
      "   template = Template(template_string)\n",
      "   ```\n",
      "\n",
      "3. **Pass data to the template:**\n",
      "   Now, pass the user's name and tasks to the `render` method:\n",
      "\n",
      "   ```python\n",
      "   user_name = \"Alice\"\n",
      "   tasks = [\"Buy groceries\", \"Finish project\"]\n",
      "\n",
      "   # Render the template\n",
      "   rendered_message = template.render(user_name=user_name, task1=task1, task2=task2)\n",
      "   ```\n",
      "\n",
      "4. **Output the rendered message:**\n",
      "   The `rendered_message` will now contain the dynamic output based on the provided data.\n",
      "\n",
      "   ```python\n",
      "   print(rendered_message)\n",
      "   ```\n",
      "\n",
      "Output:\n",
      "```\n",
      "Welcome to our system, Alice!\n",
      "Your tasks today are:\n",
      "- Buy groceries\n",
      "- Finish project\n",
      "```\n",
      "\n",
      "This demonstrates a simple chat templating example using `jinja2`. You can expand this to more complex templates as needed. If you're using a different chat platform or library, the process might vary slightly, but the concept is similar.\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785d370c26b32444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:16.935516Z",
     "start_time": "2024-03-12T20:52:16.913694Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading output from /lfs/skampere1/0/kaif/model_outputs/Deepseek-7b-Instruct/Algebra/predictions2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/model_outputs/Deepseek-7b-Instruct/Counting_and_Probability/predictions2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/model_outputs/Deepseek-7b-Instruct/Geometry/predictions2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/model_outputs/Deepseek-7b-Instruct/Intermediate_Algebra/predictions2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/model_outputs/Deepseek-7b-Instruct/Number_Theory/predictions2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/model_outputs/Deepseek-7b-Instruct/Prealgebra/predictions2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/model_outputs/Deepseek-7b-Instruct/Precalculus/predictions2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/model_outputs/Deepseek-7b-Instruct/GSM8K/predictions2.pkl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path(\"~/model_outputs\").expanduser()\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "load = True\n",
    "predictions = {\n",
    "    dataset: generate(\n",
    "        [p['question'] for p in data], llm, tokenizer, sampling_params, load=load, override=True,\n",
    "        output_path=save_dir / MODEL_MAP[\"deepseek-ai/deepseek-math-7b-instruct\"] / dataset / \"predictions2.pkl\"\n",
    "    ) for dataset, data in datasets_subset.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "322373ed9f2ab5c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T05:59:17.432861Z",
     "start_time": "2024-03-12T05:59:17.418255Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RequestOutput(request_id=0, prompt='Answer the following 4 questions:\\n\\n1. Let \\\\[f(x) = \\\\left\\\\{\\n\\\\begin{array}{cl} ax+3, &\\\\text{ if }x>2, \\\\\\\\\\nx-5 &\\\\text{ if } -2 \\\\le x \\\\le 2, \\\\\\\\\\n2x-b &\\\\text{ if } x <-2.\\n\\\\end{array}\\n\\\\right.\\\\]Find $a+b$ if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).\\n\\nFor the piecewise function to be continuous, the cases must \"meet\" at $2$ and $-2$. For example, $ax+3$ and $x-5$ must be equal when $x=2$. This implies $a(2)+3=2-5$, which we solve to get $2a=-6 \\\\Rightarrow a=-3$. Similarly, $x-5$ and $2x-b$ must be equal when $x=-2$. Substituting, we get $-2-5=2(-2)-b$, which implies $b=3$. So $a+b=-3+3=\\\\boxed{0}$.\\n\\n\\n2. A rectangular band formation is a formation with $m$ band members in each of $r$ rows, where $m$ and $r$ are integers. A particular band has less than 100 band members. The director arranges them in a rectangular formation and finds that he has two members left over. If he increases the number of members in each row by 1 and reduces the number of rows by 2, there are exactly enough places in the new formation for each band member. What is the largest number of members the band could have?\\n\\nLet $x$ be the number of band members in each row for the original formation, when two are left over.  Then we can write two equations from the given information: $$rx+2=m$$ $$(r-2)(x+1)=m$$ Setting these equal, we find: $$rx+2=(r-2)(x+1)=rx-2x+r-2$$ $$2=-2x+r-2$$ $$4=r-2x$$ We know that the band has less than 100 members.  Based on the first equation, we must have $rx$ less than 98.  We can guess and check some values of $r$ and $x$ in the last equation.  If $r=18$, then $x=7$, and $rx=126$ which is too big.  If $r=16$, then $x=6$, and $rx=96$, which is less than 98.  Checking back in the second formation, we see that $(16-2)(6+1)=14\\\\cdot 7=98$ as it should.  This is the best we can do, so the largest number of members the band could have is $\\\\boxed{98}$.\\n\\n\\n3. What is the degree of the polynomial $(4 +5x^3 +100 +2\\\\pi x^4 + \\\\sqrt{10}x^4 +9)$?\\n\\nThis polynomial is not written in standard form.  However, we don\\'t need to write it in standard form, nor do we need to pay attention to the coefficients.  We just look for the exponents on $x$.  We have an $x^4$ term and no other term of higher degree, so $\\\\boxed{4}$ is the degree of the polynomial.\\n\\n\\n4. The sum of the squares of two positive integers is 193. The product of the two integers is 84. What is the sum of the two integers?', prompt_token_ids=[100000, 32349, 254, 1893, 207, 19, 4313, 25, 185, 185, 16, 13, 3960, 25911, 69, 7, 87, 8, 403, 357, 1354, 7298, 185, 59, 951, 90, 2492, 1061, 538, 92, 7624, 10, 18, 11, 8912, 822, 90, 565, 615, 87, 29, 17, 11, 1919, 185, 87, 12, 20, 8912, 822, 90, 565, 615, 570, 17, 357, 275, 1376, 357, 275, 207, 17, 11, 1919, 185, 17, 87, 12, 65, 8912, 822, 90, 565, 615, 1376, 20359, 17, 13, 185, 59, 409, 90, 2492, 92, 185, 59, 1035, 50575, 12971, 363, 64, 10, 65, 3, 565, 254, 64388, 1157, 317, 9344, 334, 3262, 2456, 344, 895, 4150, 481, 330, 10237, 1673, 26947, 520, 30602, 473, 254, 3322, 633, 185, 185, 1988, 254, 64388, 1157, 276, 330, 9344, 11, 254, 3863, 1534, 440, 44146, 1, 430, 363, 17, 3, 285, 15718, 17, 1332, 1494, 2203, 11, 363, 1100, 10, 18, 3, 285, 363, 87, 12, 20, 3, 1534, 330, 5929, 754, 363, 87, 28, 17, 1332, 1002, 8266, 363, 64, 7, 17, 9759, 18, 28, 17, 12, 20, 1348, 588, 395, 8708, 276, 752, 363, 17, 64, 10196, 21, 357, 31227, 245, 10196, 18, 1332, 21367, 11, 363, 87, 12, 20, 3, 285, 363, 17, 87, 12, 65, 3, 1534, 330, 5929, 754, 363, 87, 10196, 17, 1332, 37994, 29878, 11, 395, 752, 15718, 17, 12, 20, 28, 17, 7543, 17, 8226, 65, 1348, 588, 8266, 363, 65, 28, 18, 1332, 2086, 363, 64, 10, 65, 10196, 18, 10, 18, 1928, 63962, 90, 15, 3013, 185, 185, 185, 17, 13, 338, 22069, 4219, 12169, 317, 245, 12169, 366, 363, 76, 3, 4219, 4000, 279, 1319, 280, 363, 81, 3, 7713, 11, 1066, 363, 76, 3, 285, 363, 81, 3, 418, 26559, 13, 338, 2590, 4219, 643, 2246, 853, 207, 16, 15, 15, 4219, 4000, 13, 429, 7066, 5382, 7935, 768, 279, 245, 22069, 12169, 285, 13373, 344, 362, 643, 984, 4000, 2116, 855, 13, 1273, 362, 9664, 254, 1604, 280, 4000, 279, 1319, 5451, 457, 207, 16, 285, 16438, 254, 1604, 280, 7713, 457, 207, 17, 11, 745, 418, 5096, 2527, 6171, 279, 254, 761, 12169, 327, 1319, 4219, 5612, 13, 2461, 317, 254, 8397, 1604, 280, 4000, 254, 4219, 1027, 463, 30, 185, 185, 4160, 363, 87, 3, 330, 254, 1604, 280, 4219, 4000, 279, 1319, 5451, 327, 254, 3632, 12169, 11, 754, 984, 418, 2116, 855, 13, 207, 2928, 395, 481, 3708, 984, 8935, 473, 254, 2028, 1757, 25, 12900, 22585, 10, 17, 28, 76, 3539, 363, 9313, 81, 12, 17, 4951, 87, 10, 16, 4576, 76, 3539, 28204, 1069, 5929, 11, 395, 1275, 25, 12900, 22585, 10, 17, 10527, 81, 12, 17, 4951, 87, 10, 16, 4576, 22585, 12, 17, 87, 10, 81, 12, 17, 3539, 12900, 17, 10196, 17, 87, 10, 81, 12, 17, 3539, 12900, 19, 28, 81, 12, 17, 87, 3539, 1003, 1006, 344, 254, 4219, 643, 2246, 853, 207, 16, 15, 15, 4000, 13, 207, 19851, 331, 254, 1022, 6337, 11, 395, 1534, 463, 363, 22585, 3, 2246, 853, 207, 24, 23, 13, 207, 1003, 481, 6384, 285, 2341, 742, 3039, 280, 363, 81, 3, 285, 363, 87, 3, 279, 254, 1562, 6337, 13, 207, 1273, 363, 81, 28, 16, 23, 1348, 937, 363, 87, 28, 22, 1348, 285, 363, 22585, 28, 16, 17, 21, 3, 588, 317, 1653, 2567, 13, 207, 1273, 363, 81, 28, 16, 21, 1348, 937, 363, 87, 28, 21, 1348, 285, 363, 22585, 28, 24, 21, 1348, 588, 317, 2246, 853, 207, 24, 23, 13, 207, 59987, 1062, 279, 254, 1864, 12169, 11, 395, 1019, 344, 3309, 16, 21, 12, 17, 4951, 21, 10, 16, 4576, 16, 19, 59, 3560, 207, 22, 28, 24, 23, 3, 372, 359, 1023, 13, 207, 1002, 317, 254, 1608, 395, 481, 536, 11, 558, 254, 8397, 1604, 280, 4000, 254, 4219, 1027, 463, 317, 628, 63962, 90, 24, 23, 3013, 185, 185, 185, 18, 13, 2461, 317, 254, 5757, 280, 254, 15968, 3309, 19, 919, 20, 87, 61, 18, 919, 16, 15, 15, 919, 17, 59, 1804, 1376, 61, 19, 919, 357, 4221, 90, 16, 15, 92, 87, 61, 19, 919, 24, 1026, 30, 185, 185, 1567, 15968, 317, 441, 4473, 279, 4182, 1020, 13, 207, 3159, 11, 395, 1113, 6, 83, 933, 276, 3708, 359, 279, 4182, 1020, 11, 5068, 536, 395, 933, 276, 2622, 5379, 276, 254, 14209, 13, 207, 1003, 953, 1068, 327, 254, 51995, 331, 363, 87, 1332, 207, 1003, 463, 274, 363, 87, 61, 19, 3, 1639, 285, 642, 750, 1639, 280, 3872, 5757, 11, 558, 628, 63962, 90, 19, 759, 317, 254, 5757, 280, 254, 15968, 13, 185, 185, 185, 19, 13, 429, 2555, 280, 254, 26532, 280, 984, 4864, 26559, 317, 207, 16, 24, 18, 13, 429, 1943, 280, 254, 984, 26559, 317, 207, 23, 19, 13, 2461, 317, 254, 2555, 280, 254, 984, 26559, 30], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n\\nLet the two positive integers be $x$ and $y$. We are given that $x^2 + y^2 = 193$ and $xy = 84$.\\n\\nWe want to find the sum of the two integers, $x+y$. We know that $(x+y)^2 = x^2 + 2xy + y^2$. We can substitute the given values into this equation to find $x+y$.\\n\\nSubstituting $x^2 + y^2 = 193$ and $xy = 84$ into the equation, we get $(x+y)^2 = 193 + 2 \\\\cdot 84 = 361$.\\n\\nTaking the square root of both sides, we find that $x+y = \\\\sqrt{361} = 19$.\\n\\nTherefore, the sum of the two integers is $\\\\boxed{19}$.', token_ids=[185, 185, 4160, 254, 984, 4864, 26559, 330, 363, 87, 3, 285, 363, 88, 1332, 1003, 418, 2028, 344, 363, 87, 61, 17, 919, 320, 61, 17, 403, 207, 16, 24, 18, 3, 285, 363, 5087, 403, 207, 23, 19, 1332, 185, 185, 1380, 1121, 276, 1275, 254, 2555, 280, 254, 984, 26559, 11, 363, 87, 10, 88, 1332, 1003, 1006, 344, 3309, 87, 10, 88, 5317, 17, 403, 1376, 61, 17, 919, 207, 17, 5087, 919, 320, 61, 17, 1332, 1003, 481, 22463, 254, 2028, 3039, 881, 437, 6337, 276, 1275, 363, 87, 10, 88, 1332, 185, 185, 81089, 363, 87, 61, 17, 919, 320, 61, 17, 403, 207, 16, 24, 18, 3, 285, 363, 5087, 403, 207, 23, 19, 3, 881, 254, 6337, 11, 395, 752, 3309, 87, 10, 88, 5317, 17, 403, 207, 16, 24, 18, 919, 207, 17, 357, 3560, 207, 23, 19, 403, 207, 18, 21, 16, 1332, 185, 185, 33355, 254, 7742, 4343, 280, 1572, 9226, 11, 395, 1275, 344, 363, 87, 10, 88, 403, 357, 4221, 90, 18, 21, 16, 92, 403, 207, 16, 24, 1332, 185, 185, 14494, 11, 254, 2555, 280, 254, 984, 26559, 317, 628, 63962, 90, 16, 24, 3013, 100001], cumulative_logprob=-27.48089710250497, logprobs=None, finish_reason=stop)], finished=True, metrics=RequestMetrics(arrival_time=7523255.291576873, last_token_time=7523255.291576873, first_scheduled_time=1709413067.3025672, first_token_time=1709413067.4208841, time_in_queue=1701889812.0109904, finished_time=1709413080.9586775), lora_request=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['Algebra'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a32b234932d21fcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:20.806389Z",
     "start_time": "2024-03-12T20:52:20.778255Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import get_boxed_answer\n",
    "from latex_formater import latex_deformat\n",
    "import re\n",
    "\n",
    "\n",
    "def grade(answers: list[str], targets: list[str]) -> list:\n",
    "    return [a == t for a, t in zip(answers, targets)]\n",
    "\n",
    "\n",
    "def grade_predictions(outputs, data, dataset):\n",
    "    if dataset == 'GSM8K':\n",
    "        boxed_predictions = []\n",
    "        for o in outputs:\n",
    "            try:\n",
    "                boxed_predictions.append(latex_deformat(o[-1]['content']))\n",
    "                # remove any characters that are not numbers\n",
    "                boxed_predictions = [re.sub(r'[^0-9]', '', p) for p in boxed_predictions]\n",
    "            except:\n",
    "                # print(get_boxed_answer(o.outputs[0].text))\n",
    "                print(o[-1]['content'])\n",
    "        # use regex to extract the answer\n",
    "        boxed_answers = [latex_deformat(re.search(r'\\n#### (.*)', d).group(1)) for d in data]\n",
    "        boxed_answers = [re.sub(r'[^0-9]', '', p) for p in boxed_answers]\n",
    "\n",
    "        grades = grade(boxed_predictions, boxed_answers)\n",
    "        return grades\n",
    "    else:\n",
    "        boxed_predictions = []\n",
    "        for o in outputs:\n",
    "            try:\n",
    "                # boxed_predictions.append(latex_deformat(get_boxed_answer(o.outputs[0].text)))\n",
    "                boxed_predictions.append(latex_deformat(get_boxed_answer(o[-1]['content'])))\n",
    "            except:\n",
    "                # print(get_boxed_answer(o.outputs[0].text))\n",
    "                print(o[-1]['content'])\n",
    "        # boxed_predictions = [latex_deformat(get_boxed_answer(o.outputs[0].text)) for o in outputs]\n",
    "        boxed_answers = [latex_deformat(get_boxed_answer(d)) for d in data]\n",
    "        grades = grade(boxed_predictions, boxed_answers)\n",
    "\n",
    "        return grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918187a9c18df4ef",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-10T05:12:36.237600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "#\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "#\n",
    "#     {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "#\n",
    "#     {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "# ]\n",
    "# tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ba29985d2299904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T04:20:22.628905Z",
     "start_time": "2024-03-11T04:20:22.615632Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': \"Patricia has 4 barrels of 7 gallons each. All of them are empty, and there's only one faucet with a flow rate of 3.5 gallons per minute. How many minutes will it take to fill all 4 barrels?\"}, {'role': 'assistant', 'content': 'Each barrel is 7 gallons and Patricia has 4 barrels, so the total volume to be filled is 7 gallons/barrel * 4 barrels = 28 gallons.\\n\\nThe faucet has a flow rate of 3.5 gallons per minute. To find out how long it takes to fill 28 gallons, we divide the total volume by the flow rate:\\n\\n28 gallons / 3.5 gallons/minute = 8 minutes.\\n\\nSo, it will take 8 minutes to fill all 4 barrels.'}, {'role': 'user', 'content': 'FINAL ANSWER: '}, {'role': 'assistant', 'content': '8 minutes'}]\n",
      "==================================================\n",
      "How many gallons does Patricia need? ** If Patricia has 4 barrels and each barrel has 7 gallons, then in total she needs 4*7=<<4*7=28>>28 gallons\n",
      "How many minutes will it take to fill all 4 barrels? ** If the faucet can provide 3.5 gallons every minute, then she will need 28/3.5=<<28/3.5=8>>8 minutes to fill all 4 barrels\n",
      "#### 8\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "dataset_name = 'GSM8K'\n",
    "# print(predictions[dataset_name][i].prompt)\n",
    "# print(predictions[dataset_name][i].outputs[0].text)\n",
    "# print('=' * 50)\n",
    "# print(datasets_subset[dataset_name][i]['answer'])\n",
    "print(predictions[dataset_name][i][-4:])\n",
    "print('=' * 50)\n",
    "print(datasets_subset[dataset_name][i]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2d71bff147cf592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:46:07.224932Z",
     "start_time": "2024-03-12T20:46:07.207368Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algebra': 0.58,\n",
       " 'Counting_and_Probability': 0.33,\n",
       " 'Geometry': 0.27,\n",
       " 'Intermediate_Algebra': 0.24,\n",
       " 'Number_Theory': 0.42,\n",
       " 'Prealgebra': 0.61,\n",
       " 'Precalculus': 0.33,\n",
       " 'GSM8K': 0.89}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grades = {\n",
    "    dataset: grade_predictions(predictions[dataset], [p['answer'] for p in data], dataset)\n",
    "    for dataset, data in datasets_subset.items()\n",
    "}\n",
    "{dataset: np.mean(grade) for dataset, grade in grades.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e69a2cbe85337ffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:24.930216Z",
     "start_time": "2024-03-12T20:52:24.893977Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fix_mistake_prompt = [\n",
    "    [\"There is a mistake in this response. Explain the mistake.\",\n",
    "     \"Craft a response that doesn't make this mistake. Do not apologize for the mistake or make any similar comment. Just create a new response that doesn't make the mistake that can replace the old response.\"],\n",
    "    [\"What is wrong with this answer?\",\n",
    "     \"Now construct a response that fixes this mistake. Do not apologize for the mistake. Simply answer the question correctly without making any other comments.\"]\n",
    "]\n",
    "\n",
    "fix_mistake_prompts = [\n",
    "    {\n",
    "        dataset:\n",
    "            [\n",
    "                [\n",
    "                    {\"role\": \"user\", \"content\": o[-4]['content']},\n",
    "                    {\"role\": \"assistant\", \"content\": o[-3]['content']},\n",
    "                ] + [\n",
    "                    {\"role\": \"user\", \"content\": p}\n",
    "                    for p in fix_mistake_prompt[cp_idx]\n",
    "                ]\n",
    "                for o in predictions[dataset]\n",
    "            ]\n",
    "        for dataset, data in datasets_subset.items()\n",
    "    }\n",
    "    for cp_idx in range(len(fix_mistake_prompt))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ae130d7844081b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:46:12.614561Z",
     "start_time": "2024-03-12T20:46:12.580984Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ask how confident the model is in its predictions\n",
    "confidence_prompt = [\n",
    "    \"How confident are you this is correct? Explain your reasoning. Give a 1-5 rating, 1 being not confident at all and 5 being very confident. End your response with 'Rating: ' followed by your rating.\",\n",
    "    \"Are you sure this is correct? Give your confidence as a percentage from 0 to 100.\\n\\nConfidence: \",\n",
    "    \"Are you sure this is correct? Explain your reasoning and give your confidence as a percentage from 0% to 100%.\",\n",
    "    \"How confident are you that this proof is right? Explain your reasoning and say whether you are very confident, confident, somewhat confident, not very confident, or not confident.\",\n",
    "    \"Above is a student response to a question. Pretend you are a teacher grading the response. How confident are you that the student's response is correct? Explain your reasoning and give your confidence as a percentage from 0% to 100%.\",\n",
    "    \"Above is a student response to a question on a test. You are an expert and nuanced grader who has a great intuition for these things. How confident are you that the student's response is correct? Explain your reasoning and give your confidence as a percentage from 0% to 100%.\",\n",
    "    \"Above is a student response to a question on a test. Are there any mistakes in the student's response? If so, explain the mistake and say the student's response is incorrect. If not, say the student's response is correct.\",\n",
    "    \"Are there any mistakes in this answer? If so, explain the mistake and say the student's response is incorrect. If not, say the student's response is correct.\",\n",
    "    \"Are you sure this is correct? Explain your reasoning and give your confidence as a percentage from 0% to 100%.\",\n",
    "    \"How confident are you that there is a mistake in this answer? Explain your reasoning and give your confidence as a percentage from 0% to 100%.\",\n",
    "]\n",
    "\n",
    "# cp_idx = 1\n",
    "\n",
    "\n",
    "# concat the confidence prompts to the original prompts and the model's predictions\n",
    "confidence_prompts = [\n",
    "    {\n",
    "        dataset:\n",
    "            [\n",
    "                [\n",
    "                    {\"role\": \"user\", \"content\": o[-4]['content']},\n",
    "                    {\"role\": \"assistant\", \"content\": o[-3]['content']},\n",
    "                    {\"role\": \"user\", \"content\": confidence_prompt[cp_idx]}\n",
    "                ]\n",
    "                for o in predictions[dataset]\n",
    "            ]\n",
    "        for dataset, data in datasets_subset.items()\n",
    "    }\n",
    "    for cp_idx in range(len(confidence_prompt))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e943a2ebb5c30823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T01:27:50.308945Z",
     "start_time": "2024-03-11T01:27:50.298328Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'What is the product of the solutions of the equation $-35=-x^2-2x?$\\n\\nWrap your final answer with the `\\\\boxed{}` command.'}, {'role': 'assistant', 'content': \"To find the product of the solutions of the quadratic equation $-35 = -x^2 - 2x$, we first need to rearrange the equation and then solve for $x$. \\n\\nStarting with:\\n\\\\[ -35 = -x^2 - 2x \\\\]\\n\\nAdding $x^2$ to both sides to get rid of the negative on the left side:\\n\\\\[ x^2 + 2x - 35 = 0 \\\\]\\n\\nNow, we can factor the quadratic or use the quadratic formula. Since this is a quadratic in standard form, it's straightforward to factor:\\n\\n\\\\[ (x + 7)(x - 5) = 0 \\\\]\\n\\nThe solutions occur when the factors equal zero, so we have:\\n\\\\[ x + 7 = 0 \\\\quad \\\\text{or} \\\\quad x - 5 = 0 \\\\]\\n\\nSolving for $x$ gives us:\\n\\\\[ x_1 = -7 \\\\quad \\\\text{and} \\\\quad x_2 = 5 \\\\]\\n\\nThe product of the solutions is:\\n\\\\[ \\\\boxed{x_1 \\\\cdot x_2 = (-7) \\\\cdot 5 = -35} \\\\]\"}, {'role': 'user', 'content': \"There is a mistake in the student's response. Explain the mistake.\"}, {'role': 'user', 'content': \"Craft a response that doesn't make this mistake.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(fix_mistake_prompts[0]['Algebra'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd5f855718795a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:56:03.292441Z",
     "start_time": "2024-03-12T20:52:33.400769Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 25.45it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.23it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:19<00:00,  5.06it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.48it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 16.66it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.60it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.98it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.36it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.53it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.70it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 28.35it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.57it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.74it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 35.83it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.36it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 29.14it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.28it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 24.24it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.97it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 16.77it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:07<00:00, 14.08it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 23.60it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.20it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.61it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.25it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 26.66it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.52it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.78it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 37.69it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 23.95it/s]\n"
     ]
    }
   ],
   "source": [
    "load = True\n",
    "fix_mistake_predictions = [\n",
    "    {\n",
    "        dataset: generate(\n",
    "            prompts, llm, tokenizer, sampling_params, load=load, override=True,\n",
    "            output_path=save_dir / MODEL_MAP[model_name] / dataset / f\"fix_mistake_{cp_idx}.pkl\"\n",
    "        ) for dataset, prompts in fix_mistake_prompts[cp_idx].items()\n",
    "    }\n",
    "    for cp_idx in range(len(fix_mistake_prompt))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "25bec68b6e19f355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T01:07:18.352135Z",
     "start_time": "2024-03-11T00:59:17.302821Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Algebra_Qwen-7b_confidence_predictions_0.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Counting and Probability_Qwen-7b_confidence_predictions_0.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Geometry_Qwen-7b_confidence_predictions_0.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Intermediate Algebra_Qwen-7b_confidence_predictions_0.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Number Theory_Qwen-7b_confidence_predictions_0.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Prealgebra_Qwen-7b_confidence_predictions_0.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Precalculus_Qwen-7b_confidence_predictions_0.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/GSM8K_Qwen-7b_confidence_predictions_0.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Algebra_Qwen-7b_confidence_predictions_1.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Counting and Probability_Qwen-7b_confidence_predictions_1.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Geometry_Qwen-7b_confidence_predictions_1.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Intermediate Algebra_Qwen-7b_confidence_predictions_1.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Number Theory_Qwen-7b_confidence_predictions_1.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Prealgebra_Qwen-7b_confidence_predictions_1.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Precalculus_Qwen-7b_confidence_predictions_1.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/GSM8K_Qwen-7b_confidence_predictions_1.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Algebra_Qwen-7b_confidence_predictions_2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Counting and Probability_Qwen-7b_confidence_predictions_2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Geometry_Qwen-7b_confidence_predictions_2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Intermediate Algebra_Qwen-7b_confidence_predictions_2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Number Theory_Qwen-7b_confidence_predictions_2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Prealgebra_Qwen-7b_confidence_predictions_2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Precalculus_Qwen-7b_confidence_predictions_2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/GSM8K_Qwen-7b_confidence_predictions_2.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Algebra_Qwen-7b_confidence_predictions_3.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Counting and Probability_Qwen-7b_confidence_predictions_3.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Geometry_Qwen-7b_confidence_predictions_3.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Intermediate Algebra_Qwen-7b_confidence_predictions_3.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Number Theory_Qwen-7b_confidence_predictions_3.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Prealgebra_Qwen-7b_confidence_predictions_3.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Precalculus_Qwen-7b_confidence_predictions_3.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/GSM8K_Qwen-7b_confidence_predictions_3.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Algebra_Qwen-7b_confidence_predictions_4.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Counting and Probability_Qwen-7b_confidence_predictions_4.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Geometry_Qwen-7b_confidence_predictions_4.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Intermediate Algebra_Qwen-7b_confidence_predictions_4.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Number Theory_Qwen-7b_confidence_predictions_4.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Prealgebra_Qwen-7b_confidence_predictions_4.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Precalculus_Qwen-7b_confidence_predictions_4.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/GSM8K_Qwen-7b_confidence_predictions_4.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Algebra_Qwen-7b_confidence_predictions_5.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Counting and Probability_Qwen-7b_confidence_predictions_5.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Geometry_Qwen-7b_confidence_predictions_5.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Intermediate Algebra_Qwen-7b_confidence_predictions_5.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Number Theory_Qwen-7b_confidence_predictions_5.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Prealgebra_Qwen-7b_confidence_predictions_5.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Precalculus_Qwen-7b_confidence_predictions_5.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/GSM8K_Qwen-7b_confidence_predictions_5.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Algebra_Qwen-7b_confidence_predictions_6.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Counting and Probability_Qwen-7b_confidence_predictions_6.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Geometry_Qwen-7b_confidence_predictions_6.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Intermediate Algebra_Qwen-7b_confidence_predictions_6.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Number Theory_Qwen-7b_confidence_predictions_6.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Prealgebra_Qwen-7b_confidence_predictions_6.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/Precalculus_Qwen-7b_confidence_predictions_6.pkl\n",
      "Loading output from /lfs/skampere1/0/kaif/generated_predictions/GSM8K_Qwen-7b_confidence_predictions_6.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.97it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.75it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.39it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  5.96it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.99it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.29it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:17<00:00,  5.69it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.67it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:26<00:00,  3.77it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:25<00:00,  3.89it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:28<00:00,  3.53it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:33<00:00,  2.96it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:26<00:00,  3.74it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.70it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:35<00:00,  2.85it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.85it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:19<00:00,  5.09it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.80it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:25<00:00,  3.91it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.93it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.45it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:24<00:00,  4.14it/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.94it/s]\n"
     ]
    }
   ],
   "source": [
    "load = True\n",
    "confidence_predictions = [\n",
    "    {\n",
    "        dataset: generate(\n",
    "            prompts, llm, tokenizer, sampling_params, load=load,\n",
    "            output_path=save_dir / MODEL_MAP[model_name] / dataset / f\"confidence_predictions_{cp_idx}.pkl\"\n",
    "        ) for dataset, prompts in confidence_prompts[cp_idx].items()\n",
    "    }\n",
    "    for cp_idx in range(len(confidence_prompt))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c90062d93a7f5ebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:50:09.103044Z",
     "start_time": "2024-03-12T20:50:08.970842Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algebra': 0.58,\n",
       " 'Counting_and_Probability': 0.33,\n",
       " 'Geometry': 0.27,\n",
       " 'Intermediate_Algebra': 0.24,\n",
       " 'Number_Theory': 0.42,\n",
       " 'Prealgebra': 0.61,\n",
       " 'Precalculus': 0.33,\n",
       " 'GSM8K': 0.89}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades = {\n",
    "    dataset: grade_predictions(predictions[dataset], [p['answer'] for p in data], dataset)\n",
    "    for dataset, data in datasets_subset.items()\n",
    "}\n",
    "{dataset: np.mean(grade) for dataset, grade in grades.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdbd3215e548b05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:02:29.278598Z",
     "start_time": "2024-03-12T21:02:29.164461Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algebra': 0.01,\n",
       " 'Counting_and_Probability': 0.0,\n",
       " 'Geometry': 0.0,\n",
       " 'Intermediate_Algebra': 0.02,\n",
       " 'Number_Theory': 0.02,\n",
       " 'Prealgebra': 0.0,\n",
       " 'Precalculus': 0.0,\n",
       " 'GSM8K': 0.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades = {\n",
    "    dataset: grade_predictions(fix_mistake_predictions[0][dataset], [p['answer'] for p in data], dataset)\n",
    "    for dataset, data in datasets_subset.items()\n",
    "}\n",
    "{dataset: np.mean(grade) for dataset, grade in grades.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9843c6ed613c43d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T21:02:34.176994Z",
     "start_time": "2024-03-12T21:02:34.151790Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'What is the distance between the center of the circle with equation $x^2+y^2=-4x+6y-12$ and the point $(1,7)$?'},\n",
       " {'role': 'assistant',\n",
       "  'content': ' First, we rewrite the equation of the circle in standard form $(x-a)^2+(y-b)^2=r^2$, where $(a,b)$ is the center of the circle and $r$ is the radius. \\n\\nTo do this, we complete the square for the $x$ terms and the $y$ terms separately. \\n\\nThe $x$ terms are $x^2+4x$, and we can complete the square by adding and subtracting $(4/2)^2=4$ to get $(x+2)^2$. \\n\\nThe $y$ terms are $y^2-6y$, and we can complete the square by adding and subtracting $(6/2)^2=9$ to get $(y-3)^2$. \\n\\nSo the equation of the circle becomes $(x+2)^2+(y-3)^2=4+9+12=25$. \\n\\nTherefore, the center of the circle is $(-2,3)$. \\n\\nThe distance between the center of the circle and the point $(1,7)$ can be found using the distance formula, which is $\\\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$. \\n\\nSubstituting the coordinates of the center of the circle $(-2,3)$ and the point $(1,7)$ into the formula, we get $\\\\sqrt{(1-(-2))^2+(7-3)^2}=\\\\sqrt{3^2+4^2}=\\\\sqrt{9+16}=\\\\sqrt{25}=5$. \\n\\nSo the distance between the center of the circle and the point $(1,7)$ is $\\\\boxed{5}$.'},\n",
       " {'role': 'user',\n",
       "  'content': 'There is a mistake in this response. Explain the mistake.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'The distance between the center of the circle and the point (1,7) is not 5, it is $\\\\sqrt{(1+2)^2+(7-3)^2}=\\\\sqrt{3^2+4^2}=\\\\sqrt{9+16}=\\\\sqrt{25}=5$.'},\n",
       " {'role': 'user',\n",
       "  'content': \"Craft a response that doesn't make this mistake. Do not apologize for the mistake or make any similar comment. Just create a new response that doesn't make the mistake that can replace the old response.\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Sure, here's the corrected response:\\n\\nThe distance between the center of the circle with equation $x^2+y^2=-4x+6y-12$ and the point $(1,7)$ can be found using the distance formula, which is $\\\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$. \\n\\nSubstituting the coordinates of the center of the circle $(-2,3)$ and the point $(1,7)$ into the formula, we get $\\\\sqrt{(1-2)^2+(7-3)^2}=\\\\sqrt{(-1)^2+(4)^2}=\\\\sqrt{1+16}=\\\\sqrt{17}$. \\n\\nTherefore, the distance between the center of the circle and the point (1,7) is $\\\\sqrt{17}$.\"}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_mistake_predictions[0]['Algebra'][8]"
   ]
  },
  {
   "cell_type": "code",
   "id": "9a51e03f49cfbbc0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfee33e9c366bd4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T19:22:27.879202Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "??sns.histplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a59ac63fd88203",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T19:22:27.879362Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the confidence predictions against grades\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "num_malformed_confidence_predictions = 0\n",
    "\n",
    "\n",
    "def get_confidence(confidence_prediction):\n",
    "    # get the confidence from the confidence prediction\n",
    "    # if not found return -1\n",
    "    try:\n",
    "        confidence_part = confidence_prediction.outputs[0].text.split('\\n')[0]\n",
    "        if '%' in confidence_part:\n",
    "            return int(confidence_part.split('%')[0])\n",
    "        else:\n",
    "            return int(confidence_part)\n",
    "    except:\n",
    "        global num_malformed_confidence_predictions\n",
    "        num_malformed_confidence_predictions += 1\n",
    "        return -1\n",
    "\n",
    "\n",
    "def get_rating(confidence_prediction):\n",
    "    # get the rating from the confidence prediction\n",
    "    # if not found return -1\n",
    "    # int(o.outputs[0].text.split('Rating: ')[1])\n",
    "    try:\n",
    "        rating_part = confidence_prediction.outputs[0].text.split('Rating: ')[1]\n",
    "        return int(rating_part[0])\n",
    "    except:\n",
    "        global num_malformed_confidence_predictions\n",
    "        num_malformed_confidence_predictions += 1\n",
    "        # print(confidence_prediction.outputs[0].text)\n",
    "        # raise ValueError\n",
    "        return -1\n",
    "\n",
    "\n",
    "# one plot for each dataset\n",
    "for dataset, data in confidence_predictions.items():\n",
    "    confidences = [get_confidence(o) for o in data]\n",
    "    # distribution of confidence ratings\n",
    "    # sns.histplot(confidences)\n",
    "    # plt.title(f\"{dataset} confidence ratings\")\n",
    "    # plt.show()\n",
    "    # plot the confidence ratings against the grades\n",
    "    correct = [c for g, c in zip(grades[dataset], confidences) if c != -1 and g]\n",
    "    incorrect = [c for g, c in zip(grades[dataset], confidences) if c != -1 and not g]\n",
    "    print(np.unique(correct, return_counts=True))\n",
    "    print(np.unique(incorrect, return_counts=True))\n",
    "    print(np.unique(confidences, return_counts=True))\n",
    "    # plot categorical distribution of rating conditioned on correctness\n",
    "    # x axis is categorical, y axis is density\n",
    "    # two histograms should be interleaved, not stacked\n",
    "    sns.histplot([correct, incorrect], stat='probability', common_norm=False, bins=5, multiple='dodge', legend=True)\n",
    "\n",
    "    plt.title(f\"{dataset} confidence ratings\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84ac9fd5da8afc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T19:22:27.879438Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the predictions\n",
    "# for dataset, outputs in predictions.items():\n",
    "#     with open(save_dir / f\"{dataset.replace(' ', '-')}_{MODEL_MAP[model_name]}_predictions.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5d2ab-024e-4889-9e51-1450633a4112",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T19:22:27.880783Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(prompts, model, sampling_params, chat_mode=False):\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    if chat_mode:\n",
    "        return [prompt + o.outputs[0].text for prompt, o in zip(prompts, outputs)]\n",
    "    else:\n",
    "        return [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0cea1-0114-43af-8857-a821bb76d148",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T19:22:27.881114Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\"\"\"A board game spinner is divided into three parts labeled $A$, $B$  and $C$.\n",
    "The probability of the spinner landing on $A$ is $\\\\frac{1}{3}$ and the\n",
    "probability of the spinner landing on $B$ is $\\\\frac{5}{12}$.  What is\n",
    "the probability of the spinner landing on $C$? Express your answer as a\n",
    "common fraction. Write your final answer as \"\\\\boxed{...}\".\n",
    "\n",
    "Solution:\n",
    "\"\"\"]\n",
    "# Solution: The spinner is guaranteed to land on exactly one of the three\n",
    "# regions, so we know that the sum of the probabilities of it landing in\n",
    "# each region will be 1. If we let the probability of it landing in region\n",
    "# $C$ be $x$, we then have the equation $1 = \\\\frac{5}{12}+\\\\frac{1}{3}+x$,\n",
    "# from which we have $x=\\\\boxed{\\\\frac{1}{4}}$.\n",
    "\n",
    "# Above is a question and correct solution. What is a mistake someone might make when solving the problem?\n",
    "\n",
    "# Mistake:\"\"\"]\n",
    "outputs = generate(prompts, llm, sampling_params, chat_mode=True)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4405c2be-c045-4f7d-94c8-4dee1616dbdc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T19:22:27.881353Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts2 = [o + \"\"\"\n",
    "\n",
    "Are you sure this is correct?\n",
    "\"\"\" for o in outputs]\n",
    "outputs2 = generate(prompts2, llm, sampling_params, chat_mode=True)\n",
    "print(outputs2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb88f9-fc73-4882-a54b-9d5d1835330e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T19:22:27.881473Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts2 = [o + \"\"\"\n",
    "\n",
    "Now construct a response that makes this mistake followed \n",
    "by a teacher explaining the mistake made. Begin with \"Incorrect Solution:\" \n",
    "and the teacher corrections with \"Teacher Correction:\".\n",
    "\n",
    "Incorrect Solution: \"\"\" for o in outputs]\n",
    "outputs2 = generate(prompts2, llm, sampling_params, chat_mode=True)\n",
    "print(outputs2[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
